{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x1GI7Fo8Y7x"
      },
      "source": [
        "# **Custom Knowledge ChatGPT By Damilola Yinusa - Chat with PDFs**\n",
        "\n",
        "**By Damilola Yinusa:**  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "0.   Installs, Imports and API Keys\n",
        "1.   Loading PDFs and chunking with LangChain\n",
        "2.   Embedding text and storing embeddings\n",
        "3.   Creating retrieval function\n",
        "4.   Creating chatbot with chat memory (OPTIONAL)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q24Y-g6h-Bg0"
      },
      "source": [
        "# 0. Installs, Imports and API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk2J2sYYjTkM",
        "outputId": "07d45038-83c3-453a-be95-d75f449cf89f"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL FIRST!\n",
        "!pip install -q langchain==0.0.150 pypdf pandas matplotlib tiktoken textract transformers openai faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-uszlwN641q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import GPT2TokenizerFast\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2Buv5Y0uFr8"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"{APIKEYHERE}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLULMPXa-Hu8"
      },
      "source": [
        "# 1. Loading PDFs and chunking with LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "KH546j3nkFwX",
        "outputId": "acdfbf8a-9df8-4581-bf80-a7b13b8e7258"
      },
      "outputs": [],
      "source": [
        "# You MUST add your PDF to local files in this notebook (folder icon on left hand side of screen)\n",
        "\n",
        "# Simple method - Split by pages\n",
        "loader = PyPDFLoader(\"./MPRA_paper_16255.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "print(pages[0])\n",
        "\n",
        "# SKIP TO STEP 2 IF YOU'RE USING THIS METHOD\n",
        "chunks = pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iADY2CXNlNq9"
      },
      "outputs": [],
      "source": [
        "# Advanced method - Split by chunk\n",
        "\n",
        "# Step 1: Convert PDF to text\n",
        "import textract\n",
        "doc = textract.process(\"./MPRA_paper_16255.pdf\")\n",
        "\n",
        "# Step 2: Save to .txt and reopen (helps prevent issues)\n",
        "with open('/MPRA_paper_16255.txt', 'w') as f:\n",
        "    f.write(doc.decode('utf-8'))\n",
        "\n",
        "with open('/MPRA_paper_16255.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Step 3: Create function to count tokens\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "# Step 4: Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 24,\n",
        "    length_function = count_tokens,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ_gDkwep4q7",
        "outputId": "b02b2056-e9ca-4c27-f216-e8e4d1b82243"
      },
      "outputs": [],
      "source": [
        "# Result is many LangChain 'Documents' around 500 tokens or less (Recursive splitter sometimes allows more tokens to retain context)\n",
        "type(chunks[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "fK31bxDOpz1l",
        "outputId": "7bebeba5-22ec-48f2-b0ec-876f8379adfd"
      },
      "outputs": [],
      "source": [
        "# Quick data visualization to ensure chunking was successful\n",
        "\n",
        "# Create a list of token counts\n",
        "token_counts = [count_tokens(chunk.page_content) for chunk in chunks]\n",
        "\n",
        "# Create a DataFrame from the token counts\n",
        "df = pd.DataFrame({'Token Count': token_counts})\n",
        "\n",
        "# Create a histogram of the token count distribution\n",
        "df.hist(bins=40, )\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IlznUDK-i2m"
      },
      "source": [
        "**bold text**# 2. Embed text and store embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "92ObhTAKnZzQ",
        "outputId": "c01704be-21ce-43f2-baf7-0566ad738e57"
      },
      "outputs": [],
      "source": [
        "# Get embedding model\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Create vector database\n",
        "db = FAISS.from_documents(chunks, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LPwdGDP-nPO"
      },
      "source": [
        "# 3. Setup retrieval function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "RWP92zGg5Nb_",
        "outputId": "51e95acb-5f3b-418c-de61-a77aa7bd59a0"
      },
      "outputs": [],
      "source": [
        "# Check similarity search is working\n",
        "query = \"Evolution of Deposit Dollarization in Ssome African Countries?\"\n",
        "docs = db.similarity_search(query)\n",
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Kv_sM8G5qAo"
      },
      "outputs": [],
      "source": [
        "# Create QA chain to integrate similarity search with user queries (answer query from knowledge base)\n",
        "\n",
        "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
        "\n",
        "query = \"Who created transformers?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_nH1qoL-w--"
      },
      "source": [
        "# 5. Create chatbot with chat memory (OPTIONAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evF7_Dyhtcaf"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Create conversation chain that uses our vectordb as retriver, this also allows for chat history management\n",
        "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.1), db.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pHw5siewPNt"
      },
      "outputs": [],
      "source": [
        "chat_history = []\n",
        "\n",
        "def on_submit(_):\n",
        "    query = input_box.value\n",
        "    input_box.value = \"\"\n",
        "\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"Thank you for using yournameBOT!\")\n",
        "        return\n",
        "\n",
        "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "    chat_history.append((query, result['answer']))\n",
        "\n",
        "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
        "    display(widgets.HTML(f'<b><font color=\"blue\">DOYBOT:</font></b> {result[\"answer\"]}'))\n",
        "\n",
        "print(\"Welcome to yournameBOT! Type 'exit' to stop.\")\n",
        "\n",
        "input_box = widgets.Text(placeholder='Please enter your question:')\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
